{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b648a69-97b1-472f-b2e4-c109ddbb194a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Employee_data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c6ddb4-f1ff-42f8-ae28-11f79c2567ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "(\"Ananya\", \"HR\", 52000),\n",
    "(\"Rahul\", \"Engineering\", 65000),\n",
    "(\"Priya\", \"Engineering\", 60000),\n",
    "(\"Zoya\", \"Marketing\", 48000),\n",
    "(\"Karan\", \"HR\", 53000),\n",
    "(\"Naveen\", \"Engineering\", 70000),\n",
    "(\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd46741-ea65-4a54-b56d-82691a46432e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "performance = [\n",
    "(\"Ananya\", 2023, 4.5),\n",
    "(\"Rahul\", 2023, 4.9),\n",
    "(\"Priya\", 2023, 4.3),\n",
    "(\"Zoya\", 2023, 3.8),\n",
    "(\"Karan\", 2023, 4.1),\n",
    "(\"Naveen\", 2023, 4.7),\n",
    "(\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b5c3b8-b937-4c59-81be-5bfb7e5a191d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42648ac5-4a88-4e79-96ff-33def9b9b4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|Name  |Department |Salary|Year|Rating|Project         |HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|HR         |52000 |2023|4.5   |HR Portal       |120        |\n|Rahul |Engineering|65000 |2023|4.9   |Data Platform   |200        |\n|Priya |Engineering|60000 |2023|4.3   |Data Platform   |180        |\n|Zoya  |Marketing  |48000 |2023|3.8   |Campaign Tracker|100        |\n|Karan |HR         |53000 |2023|4.1   |HR Portal       |130        |\n|Naveen|Engineering|70000 |2023|4.7   |ML Pipeline     |220        |\n|Fatima|Marketing  |45000 |2023|3.9   |Campaign Tracker|90         |\n+------+-----------+------+----+------+----------------+-----------+\n\n+-----------+----------------+\n| Department|TotalHoursWorked|\n+-----------+----------------+\n|         HR|             250|\n|Engineering|             600|\n|  Marketing|             190|\n+-----------+----------------+\n\n+----------------+------------------+\n|         Project|     AverageRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joins and Advanced Aggregations\n",
    "# 1. Join employee_data , performance_data , and project_data .\n",
    "full_join = df.join(df_perf, on=\"Name\", how=\"inner\").join(df_proj, on=\"Name\", how=\"inner\")\n",
    "\n",
    "full_join.show(truncate=False)\n",
    "\n",
    "# 2. Compute total hours worked per department.\n",
    "full_join.groupBy(\"Department\").sum(\"HoursWorked\").withColumnRenamed(\"sum(HoursWorked)\", \"TotalHoursWorked\").show()\n",
    "\n",
    "# 3. Compute average rating per project.\n",
    "full_join.groupBy(\"Project\").avg(\"Rating\").withColumnRenamed(\"avg(Rating)\", \"AverageRating\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e396cb1d-7d9a-4375-b4a3-70c8cb83ef9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Ramya|         HR|  NULL|\n+------+-----------+------+\n\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n| Ramya|2023|  NULL|\n+------+----+------+\n\n+-----+----------+------+\n| Name|Department|Salary|\n+-----+----------+------+\n|Ramya|        HR|  NULL|\n+-----+----------+------+\n\n+-----+----+------+\n| Name|Year|Rating|\n+-----+----+------+\n|Ramya|2023|  NULL|\n+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Data\n",
    "# 4. Add a row to performance_data with a None rating.\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "new_row1 = spark.createDataFrame([(\"Ramya\", \"HR\", None)], df.schema)\n",
    "df = df.union(new_row1)\n",
    "\n",
    "new_row2 = spark.createDataFrame([(\"Ramya\", 2023, None)], df_perf.schema)\n",
    "df_perf = df_perf.union(new_row2)\n",
    "\n",
    "df.show()\n",
    "df_perf.show()\n",
    "\n",
    "# 5. Filter rows with null values.\n",
    "\n",
    "df.filter(col(\"Salary\").isNull()).show()\n",
    "df_perf.filter(col(\"Rating\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fe309c-1860-41ba-9965-bd62252f29e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Rating|\n+------+-----------+------+\n|Ananya|         HR|   4.5|\n| Rahul|Engineering|   4.9|\n| Priya|Engineering|   4.3|\n|  Zoya|  Marketing|   3.8|\n| Karan|         HR|   4.1|\n|Naveen|Engineering|   4.7|\n|Fatima|  Marketing|   3.9|\n| Ramya|         HR|   4.3|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Replace null ratings with the department average.\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "\n",
    "df_join = df_perf.join(df.select(\"Name\", \"Department\"), on=\"Name\", how=\"left\")\n",
    "\n",
    "dept_avg = df_join.groupBy(\"Department\").agg(avg(\"Rating\").alias(\"DeptAvg\"))\n",
    "\n",
    "final_df = df_join.join(dept_avg, on=\"Department\", how=\"left\")\n",
    "\n",
    "final_df = final_df.withColumn(\"Rating\", when(col(\"Rating\").isNull(), col(\"DeptAvg\")).otherwise(col(\"Rating\")))\n",
    "\n",
    "final_df.select(\"Name\", \"Department\", \"Rating\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570c97b1-19ed-48c6-85cc-cf3c07f14d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------------+\n|  Name|Year|Rating|PerformanceCategory|\n+------+----+------+-------------------+\n|Ananya|2023|   4.5|               Good|\n| Rahul|2023|   4.9|          Excellent|\n| Priya|2023|   4.3|               Good|\n|  Zoya|2023|   3.8|            Average|\n| Karan|2023|   4.1|               Good|\n|Naveen|2023|   4.7|          Excellent|\n|Fatima|2023|   3.9|            Average|\n| Ramya|2023|  NULL|               NULL|\n+------+----+------+-------------------+\n\n+------+----------------+-----------+-----+\n|  Name|         Project|HoursWorked|Bonus|\n+------+----------------+-----------+-----+\n|Ananya|       HR Portal|        120| 5000|\n| Rahul|   Data Platform|        200| 5000|\n| Priya|   Data Platform|        180| 5000|\n|  Zoya|Campaign Tracker|        100| 5000|\n| Karan|       HR Portal|        130| 5000|\n|Naveen|     ML Pipeline|        220|10000|\n|Fatima|Campaign Tracker|         90| 5000|\n+------+----------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Built-In Functions and UDF\n",
    "# 7. Create a column PerformanceCategory :\n",
    "# Excellent (>=4.7),\n",
    "# Good (4.0–4.69),\n",
    "# Average (<4.0)\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "perf_category = df_perf.withColumn(\n",
    "    \"PerformanceCategory\",when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when((col(\"Rating\") >= 4.0) & (col(\"Rating\") < 4.7), \"Good\")\n",
    "    .when(col(\"Rating\") < 4.0, \"Average\")\n",
    ")\n",
    "perf_category.show()\n",
    "\n",
    "# 8. Create a UDF to assign bonus:\n",
    "# If project hours > 200 →\n",
    "# 10,000\n",
    "# Else →\n",
    "# 5,000\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def assign_bonus(hours):\n",
    "    return 10000 if hours and hours > 200 else 5000\n",
    "\n",
    "bonus_udf = udf(assign_bonus, IntegerType())\n",
    "\n",
    "df_proj.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\"))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83908ef-e273-4bbc-af0e-480850a5ffe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n|  Name|  JoinDate|MonthsWorked|\n+------+----------+------------+\n|Ananya|2021-06-01|          48|\n| Rahul|2021-06-01|          48|\n| Priya|2021-06-01|          48|\n|  Zoya|2021-06-01|          48|\n| Karan|2021-06-01|          48|\n|Naveen|2021-06-01|          48|\n|Fatima|2021-06-01|          48|\n| Ramya|2021-06-01|          48|\n+------+----------+------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date and Time Functions\n",
    "# 9. Add a column JoinDate with 2021-06-01 for all, then add MonthsWorked as difference from today.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_joined = df.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
    "\n",
    "df_joined = df_joined.withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
    "\n",
    "df_joined.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()\n",
    "\n",
    "# 10. Calculate how many employees joined before 2022.\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_joined.filter(year(\"JoinDate\") < 2022).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe74ab40-c9fa-408b-a3df-64a221752a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Ramya|         HR|  NULL|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Unions\n",
    "# 11. Create another small team DataFrame and union() it with employee_data .\n",
    "# extra_employees = [\n",
    "# (\"Meena\", \"HR\", 48000),\n",
    "# (\"Raj\", \"Marketing\", 51000)]\n",
    "# Create extra employee DataFrame using existing schema\n",
    "extra_emp = [(\"Meena\", \"HR\", 48000), (\"Raj\", \"Marketing\", 51000)]\n",
    "df_extra = spark.createDataFrame(extra_emp, df.schema)\n",
    "\n",
    "df.union(df_extra).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7905cad4-fef4-4ff7-b381-c320a73127a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Saving Results\n",
    "# 12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based\n",
    "# on Department .\n",
    "\n",
    "df_join = df.join(df_perf, on=\"Name\", how=\"inner\")\n",
    "\n",
    "df_final = df_join.join(df_proj, on=\"Name\", how=\"inner\")\n",
    "df_final.show()\n",
    "# Save to Parquet partitioned by Department\n",
    "df_final.write.mode(\"overwrite\").partitionBy(\"Department\").parquet(\"/tmp/final_employee_data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Combining Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}