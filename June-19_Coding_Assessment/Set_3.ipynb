{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d5428d-160f-41dc-9220-f187898ea094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "spark = SparkSession.builder.appName(\"InventoryAlertingSystem\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"ItemID\", StringType(), True),\n",
    "    StructField(\"ItemName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Warehouse\", StringType(), True),\n",
    "    StructField(\"StockQty\", IntegerType(), True),\n",
    "    StructField(\"ReorderLevel\", IntegerType(), True),\n",
    "    StructField(\"LastRestocked\", DateType(), True),\n",
    "    StructField(\"UnitPrice\", IntegerType(), True),\n",
    "    StructField(\"Supplier\", StringType(), True)\n",
    "])\n",
    "df = spark.read.csv(\"file:/Workspace/Shared/inventory_supply.csv\", header=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b772685d-c20e-44b9-b40b-6d9a9f3ae46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a new column NeedsReorder = StockQty < ReorderLevel .\n",
    "from pyspark.sql.functions import col\n",
    "df=df.withColumn(\"NeedsReorder\", col(\"StockQty\") < col(\"ReorderLevel\"))\n",
    "df.show()\n",
    "\n",
    "# 3. Create a view of all items that need restocking.\n",
    "df.filter(col(\"NeedsReorder\") == True).show()\n",
    "\n",
    "# 4. Highlight warehouses with more than 2 such items.\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"Warehouse\").agg(count(\"*\").alias(\"ItemsNeedingRestock\")).filter(col(\"ItemsNeedingRestock\") > 2)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1995bc-9da9-4831-b360-783792c5809e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n| Supplier|AvgPrice|\n+---------+--------+\n|   AVTech| 30000.0|\n|TechWorld| 70000.0|\n|PrintFast|  8000.0|\n| FreezeIt| 25000.0|\n|  ChairCo|  6000.0|\n+---------+--------+\n\nItems below category average price:\n+------+--------+-----------+---------+-----------+---------+\n|ItemID|ItemName|   Category|UnitPrice|CategoryAvg| Supplier|\n+------+--------+-----------+---------+-----------+---------+\n|  I001|  LED TV|Electronics|    30000|    36000.0|   AVTech|\n|  I005| Printer|Electronics|     8000|    36000.0|PrintFast|\n+------+--------+-----------+---------+-----------+---------+\n\n+---------+----------+-------------+--------+\n| Supplier|TotalItems|BelowAvgItems|GoodDeal|\n+---------+----------+-------------+--------+\n|   AVTech|         1|            1|    true|\n|PrintFast|         1|            1|    true|\n|TechWorld|         1|            0|   false|\n| FreezeIt|         1|            0|   false|\n|  ChairCo|         1|            0|   false|\n+---------+----------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: Supplier Price Optimization\n",
    "# 1. Group items by Supplier and compute average price.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"Supplier\").agg(avg(\"UnitPrice\").alias(\"AvgPrice\")).show()\n",
    "\n",
    "# Step 2: Find suppliers who offer items below average price in their category\n",
    "category_avg = df.groupBy(\"Category\").agg(avg(\"UnitPrice\").alias(\"CategoryAvg\"))\n",
    "\n",
    "df_avg = df.join(category_avg, on=\"Category\")\n",
    "\n",
    "below_avg_items = df_avg.filter(col(\"UnitPrice\") < col(\"CategoryAvg\"))\n",
    "\n",
    "print(\"Items below category average price:\")\n",
    "below_avg_items.select(\"ItemID\", \"ItemName\", \"Category\", \"UnitPrice\", \"CategoryAvg\", \"Supplier\").show()\n",
    "\n",
    "# Step 3: Tag suppliers with Good Deal if >50% of their items are below market average\n",
    "\n",
    "total = df.groupBy(\"Supplier\").agg(count(\"*\").alias(\"TotalItems\"))\n",
    "\n",
    "# Count below-average items per supplier\n",
    "below = below_avg_items.groupBy(\"Supplier\").agg(count(\"*\").alias(\"BelowAvgItems\"))\n",
    "result = total.join(below, on=\"Supplier\", how=\"left\").fillna(0)\n",
    "result = result.withColumn(\"GoodDeal\", (col(\"BelowAvgItems\") / col(\"TotalItems\")) > 0.5)\n",
    "result.select(\"Supplier\", \"TotalItems\", \"BelowAvgItems\", \"GoodDeal\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def372ab-9987-48b6-9089-0814dcc22c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 3: Cost Forecasting\n",
    "# 1. Calculate TotalStockValue = StockQty * UnitPrice .\n",
    "df=df.withColumn(\"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\"))\n",
    "\n",
    "# 2. Identify top 3 highest-value items.\n",
    "df.orderBy(col(\"TotalStockValue\").desc()).limit(3).show()\n",
    "\n",
    "# 3. Export the result as a Parquet file partitioned by Warehouse\n",
    "df.write.mode(\"overwrite\").partitionBy(\"Warehouse\").parquet(\"path/inventory.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4b1560-ff49-4eba-8633-782b7308bac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n| Warehouse|ItemCount|\n+----------+---------+\n|WarehouseA|        2|\n|WarehouseC|        1|\n|WarehouseB|        2|\n+----------+---------+\n\n+----------+-----------+--------+\n| Warehouse|   Category|AvgStock|\n+----------+-----------+--------+\n|WarehouseA|Electronics|    50.0|\n|WarehouseA|  Furniture|    40.0|\n|WarehouseB|Electronics|     6.5|\n|WarehouseC| Appliances|     5.0|\n+----------+-----------+--------+\n\n+----------+----------+\n| Warehouse|TotalStock|\n+----------+----------+\n|WarehouseA|        90|\n|WarehouseC|         5|\n|WarehouseB|        13|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4: Warehouse Utilization\n",
    "# Tasks:\n",
    "# 1. Count items stored per warehouse.\n",
    "df.groupBy(\"Warehouse\").agg(count(\"*\").alias(\"ItemCount\")).show()\n",
    "\n",
    "# 2. Average stock per category in each warehouse.\n",
    "df.groupBy(\"Warehouse\", \"Category\").agg(avg(\"StockQty\").alias(\"AvgStock\")) \\\n",
    "  .orderBy(\"Warehouse\", \"Category\").show()\n",
    "\n",
    "# 3. Determine underutilized warehouses ( total stock < 100)\n",
    "from pyspark.sql.functions import sum,col\n",
    "warehouse_stock = df.groupBy(\"Warehouse\").agg(sum(\"StockQty\").alias(\"TotalStock\"))\n",
    "warehouse_stock.filter(col(\"TotalStock\") < 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72891c5-b323-409f-8701-fb57a5a6f12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation                        |operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                         |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2025-06-19 08:31:28|3359264194789707|azuser3558_mml.local@techademy.com|DELETE                           |{predicate -> [\"(StockQty#41973 = 0)\"]}                                                                                                                 |NULL|{3934916715528331}|0611-041524-ifuhl15z|1          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 169, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 169, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}     |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-19 08:31:27|3359264194789707|azuser3558_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(ItemName#41970 = Laptop)\"]}                                                                                                            |NULL|{3934916715528331}|0611-041524-ifuhl15z|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 912, numDeletionVectorsUpdated -> 0, scanTimeMs -> 408, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 2473, rewriteTimeMs -> 504}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-19 08:31:25|3359264194789707|azuser3558_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{3934916715528331}|0611-041524-ifuhl15z|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2773}                                                                                                                                                                                                                                                              |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 5: Delta Audit Trail\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "# 1. Save as Delta table retail_inventory .\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"retail_inventory\")\n",
    "\n",
    "# 2. Update stock of 'Laptop' to 20\n",
    "delta_table = DeltaTable.forName(spark, \"retail_inventory\")\n",
    "\n",
    "delta_table.update(\n",
    "    condition=\"ItemName = 'Laptop'\",\n",
    "    set={\"StockQty\": \"20\"}\n",
    ")\n",
    "\n",
    "# 3. Delete any item with StockQty = 0\n",
    "delta_table.delete(condition=\"StockQty = 0\")\n",
    "\n",
    "# 4. Describe history and query version 0 (previous state)\n",
    "spark.sql(\"DESCRIBE HISTORY retail_inventory\").show(truncate=False)\n",
    "\n",
    "version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"retail_inventory\")\n",
    "version.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801ad2aa-4735-41d1-b606-b05fe5bd9756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------------+\n|ItemID|StockQty|RestockedRecently|\n+------+--------+-----------------+\n|  I003|      40|            false|\n|  I004|       5|            false|\n|  I002|      50|             true|\n|  I001|     110|             true|\n|  I005|      18|             true|\n+------+--------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Scenario 6: Alerts from Restock Logs (Join Task)\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "df = spark.read.table(\"retail_inventory\")\n",
    "\n",
    "df = df.withColumn(\"RestockedRecently\", lit(False))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"retail_inventory\")\n",
    "\n",
    "restock_df = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/restocks_logs.csv\") \\\n",
    "    .withColumn(\"QuantityAdded\", col(\"QuantityAdded\").cast(\"int\"))\n",
    "\n",
    "# 1. Join with inventory table to update StockQty and Calculate new stock and flag RestockedRecently = true for updated items.\n",
    "inventory_df = spark.read.table(\"retail_inventory\")\n",
    "\n",
    "restock_updates = inventory_df.join(restock_df, on=\"ItemID\", how=\"inner\") \\\n",
    "    .withColumn(\"UpdatedStock\", col(\"StockQty\") + col(\"QuantityAdded\")) \\\n",
    "    .withColumn(\"RestockedRecently\", lit(True)) \\\n",
    "    .select(\"ItemID\", \"UpdatedStock\", \"RestockedRecently\")\n",
    "\n",
    "# Use MERGE INTO to update in Delta.\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"retail_inventory\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=restock_updates.alias(\"source\"),\n",
    "    condition=\"target.ItemID = source.ItemID\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"StockQty\": \"source.UpdatedStock\",\n",
    "    \"RestockedRecently\": \"source.RestockedRecently\"\n",
    "}).execute()\n",
    "spark.read.table(\"retail_inventory\").select(\"ItemID\", \"StockQty\", \"RestockedRecently\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe45580-7b99-41f2-a3de-0442bb8c4b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------+------------+---------------+\n|    ItemName|   Category|StockQty|NeedsReorder|TotalStockValue|\n+------------+-----------+--------+------------+---------------+\n|      LED TV|Electronics|      50|       false|        1500000|\n|      Laptop|Electronics|      10|        true|         700000|\n|Office Chair|  Furniture|      40|       false|         240000|\n|Refrigerator| Appliances|       5|        true|         125000|\n|     Printer|Electronics|       3|        true|          24000|\n+------------+-----------+--------+------------+---------------+\n\n+---------+------------+\n| Supplier|AvgUnitPrice|\n+---------+------------+\n|  ChairCo|      6000.0|\n|PrintFast|      8000.0|\n| FreezeIt|     25000.0|\n|   AVTech|     30000.0|\n|TechWorld|     70000.0|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Scenario 7: Report Generation with SQL Views\n",
    "# Tasks:\n",
    "# 1. Create SQL view inventory_summary with:ItemName, Category, StockQty, NeedsReorder, TotalStockValue\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "df_summary = df.withColumn(\"NeedsReorder\", col(\"StockQty\") < col(\"ReorderLevel\")) \\\n",
    "               .withColumn(\"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\")) \\\n",
    "               .select(\"ItemName\", \"Category\", \"StockQty\", \"NeedsReorder\", \"TotalStockValue\")\n",
    "\n",
    "df_summary.createOrReplaceTempView(\"inventory_summary\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM inventory_summary\").show()\n",
    "\n",
    "# 2. Create view supplier_leaderboard sorted by average price\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW supplier_leaderboard AS\n",
    "    SELECT Supplier, ROUND(AVG(UnitPrice), 2) AS AvgUnitPrice\n",
    "    FROM retail_inventory\n",
    "    GROUP BY Supplier\n",
    "    ORDER BY AvgUnitPrice ASC\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM supplier_leaderboard\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d51af1-3b4d-409a-97e9-26b1255fa034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+------------+-------------+\n|ItemID|    ItemName|StockQty|ReorderLevel|StockCategory|\n+------+------------+--------+------------+-------------+\n|  I001|      LED TV|      50|          20|  Overstocked|\n|  I002|      Laptop|      10|          15|     LowStock|\n|  I003|Office Chair|      40|          10|  Overstocked|\n|  I004|Refrigerator|       5|          10|     LowStock|\n|  I005|     Printer|       3|           5|     LowStock|\n+------+------------+--------+------------+-------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice|Supplier|NeedsReorder|StockCategory|\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|  AVTech|       false|  Overstocked|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000| ChairCo|       false|  Overstocked|\n+------+------------+-----------+----------+--------+------------+-------------+---------+--------+------------+-------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|StockCategory|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-------------+\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|     LowStock|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|     LowStock|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|     LowStock|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Scenario 8: Advanced Filtering\n",
    "# Tasks:\n",
    "# 1. Use when / otherwise to categorize items:\n",
    "# \"Overstocked\" (>2x ReorderLevel)\n",
    "# \"LowStock\"\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_categorized = df.withColumn(\n",
    "    \"StockCategory\",\n",
    "    when(col(\"StockQty\") > 2 * col(\"ReorderLevel\"), \"Overstocked\")\n",
    "    .when(col(\"StockQty\") < col(\"ReorderLevel\"), \"LowStock\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "df_categorized.select(\"ItemID\", \"ItemName\", \"StockQty\", \"ReorderLevel\", \"StockCategory\").show()\n",
    "\n",
    "# 2. Use .filter() and .where() for the same and compare.\n",
    "df_categorized.filter(col(\"StockCategory\") == \"Overstocked\").show()\n",
    "df_categorized.where(col(\"StockCategory\") == \"LowStock\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28922dd-5d2d-46e1-91f0-cc3edb39f207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+------------+--------+-----------+\n|ItemID|    ItemName|LastRestocked|RestockMonth|StockAge|StockStatus|\n+------+------------+-------------+------------+--------+-----------+\n|  I003|Office Chair|   2024-03-25|           3|     451|      Stale|\n|  I004|Refrigerator|   2024-02-20|           2|     485|      Stale|\n|  I002|      Laptop|   2024-04-01|           4|     444|      Stale|\n|  I001|      LED TV|   2024-03-15|           3|     461|      Stale|\n|  I005|     Printer|   2024-03-30|           3|     446|      Stale|\n+------+------------+-------------+------------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Scenario 9: Feature Engineering\n",
    "# Tasks:\n",
    "# 1. Extract RestockMonth from LastRestocked .\n",
    "from pyspark.sql.functions import month, current_date, datediff, when, col\n",
    "df = df.withColumn(\"RestockMonth\", month(col(\"LastRestocked\")))\n",
    "\n",
    "# 2. Create feature: StockAge = CURRENT_DATE - LastRestocked\n",
    "df = df.withColumn(\"StockAge\", datediff(current_date(), col(\"LastRestocked\")))\n",
    "\n",
    "# Step 3: Bucket StockAge into: New, Moderate, Stale\n",
    "df = df.withColumn(\n",
    "    \"StockStatus\",\n",
    "    when(col(\"StockAge\") <= 30, \"New\")\n",
    "    .when((col(\"StockAge\") > 30) & (col(\"StockAge\") <= 90), \"Moderate\")\n",
    "    .otherwise(\"Stale\")\n",
    ")\n",
    "\n",
    "df.select(\"ItemID\", \"ItemName\", \"LastRestocked\", \"RestockMonth\", \"StockAge\", \"StockStatus\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa6ea96-2bb7-4844-8e32-af2a6dc10025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Scenario 10: Export Options\n",
    "# Tasks:\n",
    "# 1. Write full DataFra\n",
    "# CSV for analysts\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"file:/Workspace/Shared/export/inventory/full_data/csv/\")\n",
    "# JSON for integration\n",
    "df.write.mode(\"overwrite\") .json(\"file:/Workspace/Shared/export/inventory/full_data/json/\")\n",
    "\n",
    "# Delta for pipelines\n",
    "df.write .format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/export/inventory/full_data/delta/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Set_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}