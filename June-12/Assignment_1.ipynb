{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28f277a-9e20-49fd-a318-2fa7f408c959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, sum as _sum, count, when\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import Row\n",
    "data = [\n",
    "    Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1}, {\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
    "    Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}], Region=\"Europe\", Amount=650.0),\n",
    "    Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2}, {\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
    "    Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}], Region=\"US\", Amount=450.0)\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(data)\n",
    "df_sales.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee83dd6-a7fc-4ffe-bed3-ab45ff0b1876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+------+-------+---+\n|OrderID|Customer|Region|Amount|Product|Qty|\n+-------+--------+------+------+-------+---+\n|101    |Ali     |Asia  |1200.0|Laptop |1  |\n|101    |Ali     |Asia  |1200.0|Mouse  |2  |\n|102    |Zara    |Europe|650.0 |Tablet |1  |\n|103    |Mohan   |Asia  |890.0 |Phone  |2  |\n|103    |Mohan   |Asia  |890.0 |Charger|1  |\n|104    |Sara    |US    |450.0 |Desk   |1  |\n+-------+--------+------+------+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Working with JSON & Nested Fields\n",
    "# 1. Flatten the Items array using explode() to create one row per product.\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_flat = df_sales.withColumn(\"Item\", explode(\"Items\")).withColumn(\"Product\", col(\"Item.Product\")) \\\n",
    "                  .withColumn(\"Qty\", col(\"Item.Qty\")) \\\n",
    "                  .drop(\"Items\", \"Item\")\n",
    "\n",
    "df_flat.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce01eb0-6d7b-4131-ae5f-f196597c31fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|Product|TotalQty|\n+-------+--------+\n| Laptop|     1.0|\n|  Mouse|     2.0|\n| Tablet|     1.0|\n|  Phone|     2.0|\n|Charger|     1.0|\n|   Desk|     1.0|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Count total quantity sold per product.\n",
    "df_flat.groupBy(\"Product\").agg(_sum(\"Qty\").alias(\"TotalQty\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1b3fa3-31d1-40d4-82d1-a08645cc291d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n|Region|NumberOfOrders|\n+------+--------------+\n|  Asia|             2|\n|Europe|             1|\n|    US|             1|\n+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Count number of orders per region.\n",
    "df_sales.groupBy(\"Region\").agg(count(\"OrderID\").alias(\"NumberOfOrders\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec88f21-75b4-44f9-902b-85c035377aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+\n|OrderID|Amount|HighValueOrder|\n+-------+------+--------------+\n|    101|1200.0|           Yes|\n|    102| 650.0|            No|\n|    103| 890.0|            No|\n|    104| 450.0|            No|\n+-------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using when and otherwise\n",
    "# 4. Create a new column HighValueOrder :\n",
    "# \"Yes\" if Amount > 1000\n",
    "# \"No\" otherwise\n",
    "df_sales_with_flag = df_sales.withColumn(\n",
    "    \"HighValueOrder\",\n",
    "    when(col(\"Amount\") > 1000, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "df_sales_with_flag.select(\"OrderID\", \"Amount\", \"HighValueOrder\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2f67f4-ce02-4e58-a3b2-b28265d8a285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+\n|OrderID|Region|ShippingZone|\n+-------+------+------------+\n|    101|  Asia|      Zone A|\n|    102|Europe|      Zone B|\n|    103|  Asia|      Zone A|\n|    104|    US|      Zone C|\n+-------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Add a column ShippingZone :\n",
    "# Asia → \"Zone A\", Europe → \"Zone B\", US → \"Zone C\"\n",
    "df_sales_with_zone = df_sales_with_flag.withColumn(\n",
    "    \"ShippingZone\",\n",
    "    when(col(\"Region\") == \"Asia\", \"Zone A\")\n",
    "    .when(col(\"Region\") == \"Europe\", \"Zone B\")\n",
    "    .when(col(\"Region\") == \"US\", \"Zone C\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "df_sales_with_zone.select(\"OrderID\", \"Region\", \"ShippingZone\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b3b7f0-b962-495b-a9d5-d1955ad1dcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         2|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\n+------+---------+\n|Region|AvgAmount|\n+------+---------+\n|  Asia|   1045.0|\n|Europe|    650.0|\n|    US|    450.0|\n+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Temporary & Permanent Views\n",
    "# 6. Register df_sales as a temporary view named sales_view .\n",
    "df_sales.createOrReplaceTempView(\"sales_view\")\n",
    "\n",
    "# 7. Write a SQL query to:\n",
    "# Count orders by Region\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Region, COUNT(*) AS OrderCount\n",
    "    FROM sales_view\n",
    "    GROUP BY Region\n",
    "\"\"\").show()\n",
    "\n",
    "# Find average amount per region\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Region, AVG(Amount) AS AvgAmount\n",
    "    FROM sales_view\n",
    "    GROUP BY Region\n",
    "\"\"\").show()\n",
    "\n",
    "# 8. Create a permanent view using saveAsTable() .\n",
    "df_sales.write.mode(\"overwrite\").saveAsTable(\"sales_permanent_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8c1901-f076-4669-bd68-8baf93b659eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+------+------+\n|OrderID|Customer|NumItems|Region|Amount|\n+-------+--------+--------+------+------+\n|    101|     Ali|       2|  Asia|1200.0|\n|    103|   Mohan|       2|  Asia| 890.0|\n+-------+--------+--------+------+------+\n\n+--------+------+\n|Customer|Amount|\n+--------+------+\n|     Ali|1200.0|\n|   Mohan| 890.0|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Use SQL to filter all orders with more than 1 item.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT OrderID, Customer, Size(Items) AS NumItems, Region, Amount\n",
    "    FROM sales_view\n",
    "    WHERE Size(Items) > 1\n",
    "\"\"\").show()\n",
    "\n",
    "# 10. Use SQL to extract customer names where Amount > 800.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Customer, Amount\n",
    "    FROM sales_view\n",
    "    WHERE Amount > 800\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc469f0-74a9-4fe1-865e-0215cae5f8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving as Parquet and Reading Again\n",
    "# 11. Save the exploded product-level DataFrame as a partitioned Parquet file by\n",
    "# Region .\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_flat = df_sales.withColumn(\"Item\", explode(\"Items\")).withColumn(\"Product\", col(\"Item.Product\")).withColumn(\"Qty\", col(\"Item.Qty\")).drop(\"Items\", \"Item\")\n",
    "\n",
    "df_flat.write.mode(\"overwrite\").partitionBy(\"Region\").parquet(\"/tmp/product_sales_by_region\")\n",
    "\n",
    "# 12. Read the parquet back and perform a group-by on Product .\n",
    "\n",
    "df_read = spark.read.parquet(\"/tmp/product_sales_by_region\")\n",
    "\n",
    "df_read.groupBy(\"Product\").sum(\"Qty\").withColumnRenamed(\"sum(Qty)\", \"TotalQty\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}