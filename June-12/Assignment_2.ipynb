{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca5a3d-f1c5-4f24-b6af-863701f3e97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|\n+------+--------+-------------------+--------+-------+-------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |\n+------+--------+-------------------+--------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "web_data = [\n",
    "Row(UserID=1, Page=\"Home\", Timestamp=\"2024-04-10 10:00:00\", Duration=35,\n",
    "Device=\"Mobile\", Country=\"India\"),\n",
    "Row(UserID=2, Page=\"Products\", Timestamp=\"2024-04-10 10:02:00\", Duration=120,\n",
    "Device=\"Desktop\", Country=\"USA\"),\n",
    "Row(UserID=3, Page=\"Cart\", Timestamp=\"2024-04-10 10:05:00\", Duration=45,\n",
    "Device=\"Tablet\", Country=\"UK\"),\n",
    "Row(UserID=1, Page=\"Checkout\", Timestamp=\"2024-04-10 10:08:00\", Duration=60,\n",
    "Device=\"Mobile\", Country=\"India\"),\n",
    "Row(UserID=4, Page=\"Home\", Timestamp=\"2024-04-10 10:10:00\", Duration=15,\n",
    "Device=\"Mobile\", Country=\"Canada\"),\n",
    "Row(UserID=2, Page=\"Contact\", Timestamp=\"2024-04-10 10:15:00\", Duration=25,\n",
    "Device=\"Desktop\", Country=\"USA\"),\n",
    "Row(UserID=5, Page=\"Products\", Timestamp=\"2024-04-10 10:20:00\", Duration=90,\n",
    "Device=\"Desktop\", Country=\"India\"),\n",
    "]\n",
    "df_web = spark.createDataFrame(web_data)\n",
    "df_web.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8489eb54-93b1-4322-a686-d9a6ba93b7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- UserID: long (nullable = true)\n |-- Page: string (nullable = true)\n |-- Timestamp: string (nullable = true)\n |-- Duration: long (nullable = true)\n |-- Device: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Display the schema of web_traffic_data .\n",
    "df_web.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70f3691-21d8-435f-9ee5-350907f8bd03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Convert the Timestamp column to a proper timestamp type.\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df_web = df_web.withColumn(\"Timestamp\", to_timestamp(\"Timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82cc94cb-f672-4953-826e-235070c8ccb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |0            |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |5            |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |8            |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |10           |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |15           |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Add a new column SessionMinute by extracting the minute from the Timestamp.\n",
    "from pyspark.sql.functions import minute\n",
    "df_web = df_web.withColumn(\"SessionMinute\", minute(\"Timestamp\"))\n",
    "df_web.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d932d2d5-4598-48f1-a812-1c5219c62a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device|Country|SessionMinute|\n+------+--------+-------------------+--------+------+-------+-------------+\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile|India  |8            |\n+------+--------+-------------------+--------+------+-------+-------------+\n\n+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering and Conditions\n",
    "# 4. Filter users who used a \"Mobile\" device and visited the \"Checkout\" page.\n",
    "df_web.filter((df_web.Device == \"Mobile\") & (df_web.Page == \"Checkout\")).show(truncate=False)\n",
    "\n",
    "# 5. Show all entries with a Duration greater than 60 seconds.\n",
    "df_web.filter(df_web.Duration > 60).show(truncate=False)\n",
    "\n",
    "# 6. Find all users from India who visited the \"Products\" page.\n",
    "df_web.filter((df_web.Country == \"India\") & (df_web.Page == \"Products\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c822d74e-159a-478b-b8e7-0be7a7bb59a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|Device |AvgDuration       |\n+-------+------------------+\n|Mobile |36.666666666666664|\n|Tablet |45.0              |\n|Desktop|78.33333333333333 |\n+-------+------------------+\n\n+-------+------------+\n|Country|SessionCount|\n+-------+------------+\n|India  |3           |\n|USA    |2           |\n|UK     |1           |\n|Canada |1           |\n+-------+------------+\n\n+----+-----+\n|Page|count|\n+----+-----+\n|Home|2    |\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregation and Grouping\n",
    "# 7. Get the average duration per device type.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_web.groupBy(\"Device\").agg(avg(\"Duration\").alias(\"AvgDuration\")).show(truncate=False)\n",
    "\n",
    "# 8. Count the number of sessions per country.\n",
    "df_web.groupBy(\"Country\").count().withColumnRenamed(\"count\", \"SessionCount\").show(truncate=False)\n",
    "\n",
    "# 9. Find the most visited page overall.\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df_web.groupBy(\"Page\").count().orderBy(desc(\"count\")).limit(1).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8953d7-f030-400d-9ec4-9399c05717ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+-------------+--------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|PageRank|\n+------+--------+-------------------+--------+-------+-------+-------------+--------+\n|1     |Home    |2024-04-10 10:00:00|35      |Mobile |India  |0            |1       |\n|1     |Checkout|2024-04-10 10:08:00|60      |Mobile |India  |8            |2       |\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |1       |\n|2     |Contact |2024-04-10 10:15:00|25      |Desktop|USA    |15           |2       |\n|3     |Cart    |2024-04-10 10:05:00|45      |Tablet |UK     |5            |1       |\n|4     |Home    |2024-04-10 10:10:00|15      |Mobile |Canada |10           |1       |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |1       |\n+------+--------+-------------------+--------+-------+-------+-------------+--------+\n\n+------+-------------+\n|UserID|TotalDuration|\n+------+-------------+\n|1     |95           |\n|3     |45           |\n|2     |145          |\n|4     |15           |\n|5     |90           |\n+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# 10. Rank each userâ€™s pages by timestamp (oldest to newest).\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"UserID\").orderBy(\"Timestamp\")\n",
    "df_web.withColumn(\"PageRank\", rank().over(window_spec)).show(truncate=False)\n",
    "\n",
    "# 11. Find the total duration of all sessions per user using groupBy .\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "df_web.groupBy(\"UserID\").agg(_sum(\"Duration\").alias(\"TotalDuration\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b78054-0080-4153-a32c-33d92e0f7d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------+-------+-------+-------------+\n|UserID|Page    |Timestamp          |Duration|Device |Country|SessionMinute|\n+------+--------+-------------------+--------+-------+-------+-------------+\n|2     |Products|2024-04-10 10:02:00|120     |Desktop|USA    |2            |\n|5     |Products|2024-04-10 10:20:00|90      |Desktop|India  |20           |\n+------+--------+-------------------+--------+-------+-------+-------------+\n\n+--------+-----------+\n|Page    |UniqueUsers|\n+--------+-----------+\n|Cart    |1          |\n|Home    |2          |\n|Checkout|1          |\n|Products|2          |\n|Contact |1          |\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Tasks\n",
    "# 12. Create a temporary view called traffic_view .\n",
    "df_web.createOrReplaceTempView(\"traffic_view\")\n",
    "\n",
    "# 13. Write a SQL query to get the top 2 longest sessions by duration.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM traffic_view\n",
    "    ORDER BY Duration DESC\n",
    "    LIMIT 2\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 14. Get the number of unique users per page using SQL.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Page, COUNT(DISTINCT UserID) AS UniqueUsers\n",
    "    FROM traffic_view\n",
    "    GROUP BY Page\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1be304-10f9-40e5-8081-23d0d95c056f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export & Save\n",
    "# 15. Save the final DataFrame to CSV.\n",
    "df_web.write.mode(\"overwrite\").option(\"header\", True).csv(\"output/web_traffic_csv\")\n",
    "\n",
    "# 16. Save partitioned by Country in Parquet format.\n",
    "df_web.write.mode(\"overwrite\").partitionBy(\"Country\").parquet(\"output/web_traffic_parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}