{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953da985-9262-40f0-8c1a-51ff01bb0acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- SubscriptionID: string (nullable = true)\n |-- UserID: string (nullable = true)\n |-- PlanType: string (nullable = true)\n |-- StartDate: date (nullable = true)\n |-- EndDate: date (nullable = true)\n |-- PriceUSD: double (nullable = true)\n |-- IsActive: boolean (nullable = true)\n |-- AutoRenew: boolean (nullable = true)\n\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\nroot\n |-- UserID: string (nullable = true)\n |-- EventTime: timestamp (nullable = true)\n |-- EventType: string (nullable = true)\n |-- FeatureUsed: string (nullable = true)\n\n+------+-------------------+---------+-----------+\n|UserID|          EventTime|EventType|FeatureUsed|\n+------+-------------------+---------+-----------+\n|  U001|2024-04-07 10:22:00|    login|  Dashboard|\n|  U002|2024-04-08 11:10:00|   upload|    Reports|\n|  U003|2024-04-09 09:45:00| download|  Analytics|\n|  U001|2024-04-10 16:00:00|   logout|  Dashboard|\n|  U004|2024-04-11 12:00:00|    login|  Dashboard|\n+------+-------------------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CourseData\").getOrCreate()\n",
    "df_sub = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/subscriptions.csv\")\n",
    "df_user = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/user_activity.csv\")\n",
    "df_sub.printSchema()\n",
    "df_sub.show()\n",
    "df_user.printSchema()\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "886391ce-161d-4483-a7aa-2a5348dd0d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A. Subscription Engagement Score (Real Metric Modeling)\n",
    "# Combine both datasets.\n",
    "# Calculate:\n",
    "# active_days = EndDate - StartDate\n",
    "# events_per_user = count(EventType) grouped by UserID\n",
    "# Create a score: engagement_score = (events_per_user / active_days) * PriceUSD\n",
    "from pyspark.sql.functions import col, datediff, count, when, sum as spark_sum\n",
    "\n",
    "df_sub = df_sub.withColumn(\"active_days\", datediff(col(\"EndDate\"), col(\"StartDate\")))\n",
    "events_per_user = df_user.groupBy(\"UserID\").agg(count(\"EventType\").alias(\"events_per_user\"))\n",
    "df_combined = df_sub.join(events_per_user, on=\"UserID\", how=\"left\")\n",
    "df_combined = df_combined.fillna({\"events_per_user\": 0})\n",
    "\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"engagement_score\", (col(\"events_per_user\") / col(\"active_days\")) * col(\"PriceUSD\")\n",
    ")\n",
    "\n",
    "df_combined.select(\"SubscriptionID\", \"UserID\", \"PlanType\", \"engagement_score\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da82385-d301-495a-b866-cbadd507b945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# B. Anomaly Detection via SQL\n",
    "# Identify users with:\n",
    "# Subscription inactive but recent activity\n",
    "# AutoRenew is true but no events in 30 days\n",
    "# Use SQL views to expose this logic.\n",
    "\n",
    "df_sub.createOrReplaceTempView(\"subscriptions\")\n",
    "df_user.createOrReplaceTempView(\"user_activity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e5772a-4b83-4acb-8f55-c510d4136a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+---------+\n|UserID|SubscriptionID|IsActive|EventTime|\n+------+--------------+--------+---------+\n+------+--------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW inactive_with_activity AS\n",
    "SELECT DISTINCT s.UserID, s.SubscriptionID, s.IsActive, a.EventTime\n",
    "FROM subscriptions s\n",
    "JOIN user_activity a ON s.UserID = a.UserID\n",
    "WHERE s.IsActive = false\n",
    "  AND a.EventTime >= CURRENT_DATE - INTERVAL '30 days'\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM inactive_with_activity\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f04a74-2315-441c-b586-1a1f3d008766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------+-------------------+\n|UserID|SubscriptionID|AutoRenew|         last_event|\n+------+--------------+---------+-------------------+\n|  U001|        SUB001|     true|2024-04-10 16:00:00|\n|  U001|        SUB004|     true|2024-04-10 16:00:00|\n+------+--------------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW auto_renew_no_activity AS\n",
    "SELECT s.UserID, s.SubscriptionID, s.AutoRenew, last_event\n",
    "FROM subscriptions s\n",
    "LEFT JOIN (\n",
    "    SELECT UserID, MAX(EventTime) AS last_event\n",
    "    FROM user_activity\n",
    "    GROUP BY UserID\n",
    ") ua ON s.UserID = ua.UserID\n",
    "WHERE s.AutoRenew = true\n",
    "  AND (ua.last_event IS NULL OR ua.last_event < current_date() - interval 30 days)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM auto_renew_no_activity\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de6221f-d124-4ef8-b60b-3cb4c5047457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+----------+--------+\n|SubscriptionID|UserID|PlanType| StartDate|PriceUSD|\n+--------------+------+--------+----------+--------+\n|        SUB003|  U003|     Pro|2024-03-10|    95.0|\n+--------------+------+--------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# C. Delta Lake + Merge Simulation\n",
    "# Imagine a billing fix needs to be applied:\n",
    "# For all Pro plans in March, increase price by $5 retroactively.\n",
    "# Use MERGE INTO on Delta table to apply the change.\n",
    "\n",
    "df_sub.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/subscriptions\")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_subs = DeltaTable.forPath(spark, \"/delta/subscriptions\")\n",
    "\n",
    "from pyspark.sql.functions import lit, month, year,col\n",
    "df_fix = df_sub.filter(\n",
    "    (col(\"PlanType\") == \"Pro\") &\n",
    "    (month(\"StartDate\") == 3) &\n",
    "    (year(\"StartDate\") == 2024)).withColumn(\"PriceUSD\", col(\"PriceUSD\") + lit(5))\n",
    "\n",
    "delta_subs.alias(\"target\").merge(\n",
    "    source=df_fix.alias(\"source\"),\n",
    "    condition=\"target.SubscriptionID = source.SubscriptionID\").whenMatchedUpdate(\n",
    "    set={\"PriceUSD\": \"source.PriceUSD\"}).execute()\n",
    "\n",
    "spark.read.format(\"delta\").load(\"/delta/subscriptions\") \\\n",
    "    .select(\"SubscriptionID\", \"UserID\", \"PlanType\", \"StartDate\", \"PriceUSD\") \\\n",
    "    .filter(\"PlanType = 'Pro' AND month(StartDate) = 3\") \\\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935902e0-36e5-41a9-bd94-4b070a7aa88e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                               |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|3      |2025-06-16 11:40:56|3359264194789707|azuser3558_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                    |NULL|{2663518146940199}|0611-041524-ifuhl15z|2          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4175, p25FileSize -> 2230, numDeletionVectorsRemoved -> 1, minFileSize -> 2230, numAddedFiles -> 1, maxFileSize -> 2230, p75FileSize -> 2230, p50FileSize -> 2230, numAddedBytes -> 2230}                                                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-16 11:40:52|3359264194789707|azuser3558_mml.local@techademy.com|MERGE    |{predicate -> [\"(SubscriptionID#2366 = SubscriptionID#23)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> []}|NULL|{2663518146940199}|0611-041524-ifuhl15z|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1950, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 5088, materializeSourceTimeMs -> 397, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2864, numTargetRowsUpdated -> 1, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1766}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-16 11:40:45|3359264194789707|azuser3558_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                      |NULL|{2663518146940199}|0611-041524-ifuhl15z|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2225}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-16 11:40:26|3359264194789707|azuser3558_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                      |NULL|{2663518146940199}|0611-041524-ifuhl15z|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2225}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n+--------------+--------+----------+--------+\n|SubscriptionID|PlanType| StartDate|PriceUSD|\n+--------------+--------+----------+--------+\n|        SUB003|     Pro|2024-03-10|    90.0|\n+--------------+--------+----------+--------+\n\n+--------------+--------+----------+--------+\n|SubscriptionID|PlanType| StartDate|PriceUSD|\n+--------------+--------+----------+--------+\n|        SUB003|     Pro|2024-03-10|    95.0|\n+--------------+--------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# D. Time Travel Debugging\n",
    "# Show describe history of the table before and after the billing fix.\n",
    "# Query using VERSION AS OF to prove the issue existed.\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_subs = DeltaTable.forPath(spark, \"/delta/subscriptions\")\n",
    "delta_subs.history().show(truncate=False)\n",
    "\n",
    "# Query version before the fix\n",
    "df_before = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/subscriptions\")\n",
    "\n",
    "df_before.filter(\"PlanType = 'Pro' AND month(StartDate) = 3\").select(\n",
    "    \"SubscriptionID\", \"PlanType\", \"StartDate\", \"PriceUSD\"\n",
    ").show()\n",
    "\n",
    "# Query current (latest) version after fix\n",
    "df_after = spark.read.format(\"delta\").load(\"/delta/subscriptions\")\n",
    "\n",
    "df_after.filter(\"PlanType = 'Pro' AND month(StartDate) = 3\").select(\n",
    "    \"SubscriptionID\", \"PlanType\", \"StartDate\", \"PriceUSD\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef44ec4-93ac-44e4-91cf-31d444b18955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+------------+-----------+\n|UserID| StartDate|PlanType|PreviousPlan|TwoStepsAgo|\n+------+----------+--------+------------+-----------+\n|  U001|2024-01-01|   basic|        NULL|       NULL|\n|  U001|2024-04-05| premium|       basic|       NULL|\n|  U002|2024-02-15|     pro|        NULL|       NULL|\n|  U003|2024-03-10|     pro|        NULL|       NULL|\n|  U004|2024-01-20|   basic|        NULL|       NULL|\n+------+----------+--------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# E. Build Tier Migration Table\n",
    "# Identify users who upgraded:\n",
    "# From Basic → Pro → Premium\n",
    "# Use PySpark with lag() function to model this.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, lower\n",
    "\n",
    "# Lowercase plan type for consistency\n",
    "df_sub = df_sub.withColumn(\"PlanType\", lower(col(\"PlanType\")))\n",
    "\n",
    "# Window spec: partition by User, ordered by StartDate\n",
    "windowSpec = Window.partitionBy(\"UserID\").orderBy(\"StartDate\")\n",
    "\n",
    "# Add lag columns\n",
    "df_migration = df_sub.withColumn(\"PreviousPlan\", lag(\"PlanType\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"TwoStepsAgo\", lag(\"PlanType\", 2).over(windowSpec))\n",
    "\n",
    "df_migration.select(\"UserID\", \"StartDate\", \"PlanType\", \"PreviousPlan\", \"TwoStepsAgo\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e136fb-677c-4ee6-a212-ba6f0279718a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------+\n|UserID|distinct_features|login_count|\n+------+-----------------+-----------+\n+------+-----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# F. Power Users Detection\n",
    "# Define a power user as:\n",
    "# Used ≥ 2 features\n",
    "# Logged in ≥ 3 times\n",
    "# Create a separate Delta table power_users\n",
    "from pyspark.sql.functions import col, countDistinct, sum, when\n",
    "\n",
    "df_user_summary = df_user.groupBy(\"UserID\").agg(\n",
    "    countDistinct(\"FeatureUsed\").alias(\"distinct_features\"),\n",
    "    sum(when(col(\"EventType\") == \"login\", 1).otherwise(0)).alias(\"login_count\")\n",
    ")\n",
    "df_power_users = df_user_summary.filter(\n",
    "    (col(\"distinct_features\") >= 2) &\n",
    "    (col(\"login_count\") >= 3)\n",
    ")\n",
    "df_power_users.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/power_users\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS power_users\")\n",
    "spark.sql(\"CREATE TABLE power_users USING DELTA LOCATION '/delta/power_users'\")\n",
    "spark.read.format(\"delta\").load(\"/delta/power_users\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf215c7-fe67-4526-aad6-3827f8efee62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+----------------------+\n|UserID|          EventTime|      NextEventTime|SessionDurationMinutes|\n+------+-------------------+-------------------+----------------------+\n|  U001|2024-04-07 10:22:00|2024-04-10 16:00:00|                4658.0|\n+------+-------------------+-------------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# G. Session Replay View\n",
    "# Build a user session trace table using:\n",
    "# Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "# Show how long each user spent between login and logout events.\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Ensure EventTime is in timestamp format\n",
    "df_user = df_user.withColumn(\"EventTime\", to_timestamp(\"EventTime\"))\n",
    "\n",
    "# Filter login and logout events\n",
    "df_filtered = df_user.filter((col(\"EventType\") == \"login\") | (col(\"EventType\") == \"logout\"))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "# Define window ordered by EventTime\n",
    "windowSpec = Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "\n",
    "\n",
    "df_session = df_filtered.withColumn(\"NextEvent\", lead(\"EventType\").over(windowSpec)) \\\n",
    "                        .withColumn(\"NextEventTime\", lead(\"EventTime\").over(windowSpec))\n",
    "\n",
    "df_sessions_only = df_session.filter((col(\"EventType\") == \"login\") & (col(\"NextEvent\") == \"logout\"))\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_session_duration = df_sessions_only.withColumn(\n",
    "    \"SessionDurationMinutes\",\n",
    "    expr(\"round((unix_timestamp(NextEventTime) - unix_timestamp(EventTime)) / 60, 2)\")\n",
    ")\n",
    "\n",
    "df_session_duration.select(\"UserID\", \"EventTime\", \"NextEventTime\", \"SessionDurationMinutes\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-16 16:55:16",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}