{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990a86a7-636e-4347-bf76-6956c9e41654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: integer (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- ProductID: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n |-- Status: string (nullable = true)\n\nroot\n |-- CustomerID: string (nullable = true)\n |-- CustomerName: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- SignupDate: date (nullable = true)\n\nroot\n |-- ProductID: string (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Stock: integer (nullable = true)\n |-- ReorderLevel: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail Analytics\").getOrCreate()\n",
    "\n",
    "orders_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/orders1.csv\")\n",
    "customers_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/customers1.csv\")\n",
    "products_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/products1.csv\")\n",
    "orders_df.printSchema()\n",
    "customers_df.printSchema()\n",
    "products_df.printSchema()\n",
    "# Save as Delta\n",
    "orders_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"/delta/orders\")\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customers\")\n",
    "products_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"/delta/products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a78c3e-a1b8-4d13-b48a-e44329c97282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|ProductID|TotalRevenue|\n+---------+------------+\n|    P1001|       75000|\n|    P1002|       50000|\n|    P1003|       30000|\n+---------+------------+\n\n+------+---------------+\n|Region|RevenueByRegion|\n+------+---------------+\n| North|         125000|\n|  West|          30000|\n+------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Write SQL to get the total revenue per Product.\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS orders USING DELTA LOCATION '/delta/orders'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS products USING DELTA LOCATION '/delta/products'\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ProductID, \n",
    "    SUM(Quantity * Price) AS TotalRevenue\n",
    "FROM orders\n",
    "WHERE Status = 'Delivered'\n",
    "GROUP BY ProductID\n",
    "ORDER BY TotalRevenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. Join Orders + Customers to find revenue by Region.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customers USING DELTA LOCATION '/delta/customers'\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.Region,\n",
    "    SUM(o.Quantity * o.Price) AS RevenueByRegion\n",
    "FROM orders o\n",
    "JOIN customers c ON o.CustomerID = c.CustomerID\n",
    "WHERE o.Status = 'Delivered'\n",
    "GROUP BY c.Region\n",
    "ORDER BY RevenueByRegion DESC\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66e453b-7b75-4513-9f25-fef7865967ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Update the Status of Pending orders to 'Cancelled'.\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "orders_delta = DeltaTable.forPath(spark, \"/delta/orders\")\n",
    "\n",
    "orders_delta.update(\n",
    "    condition=\"Status = 'Pending'\",\n",
    "    set={\"Status\": \"'Cancelled'\"}\n",
    ")\n",
    "\n",
    "# 5. Merge a new return record into Orders.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "new_return_df = spark.createDataFrame([\n",
    "    Row(OrderID=3006, CustomerID=\"C003\", ProductID=\"P1002\", Quantity=1, Price=50000, OrderDate=\"2024-05-06\", Status=\"Returned\")\n",
    "])\n",
    "\n",
    "orders_delta.alias(\"target\").merge(\n",
    "    source=new_return_df.alias(\"source\"),\n",
    "    condition=\"target.OrderID = source.OrderID\",\n",
    ").whenNotMatchedInsertAll().execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e500db-d854-4baf-80b2-d65b40da771a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>revenue_by_category</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>Category</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalRevenue</td>\n",
       "   <td>bigint</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=641763771411399#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser3558_mml.local%40techademy.com%2FJune_17_Assignment_1&redirectNotebookId=4356610334858723\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DLT Pipeline\n",
    "# 6. Create raw → cleaned → aggregated tables:\n",
    "# Clean: Remove rows with NULLs\n",
    "# Aggregated: Total revenue per Category\n",
    "import dlt\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Raw Ingest Layer\n",
    "@dlt.table(name=\"orders_raw\")\n",
    "def load_orders_raw():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"file:/Workspace/Shared/orders1.csv\")\n",
    "\n",
    "@dlt.table(name=\"products_raw\")\n",
    "def load_products_raw():\n",
    "    return spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"file:/Workspace/Shared/products1.csv\")\n",
    "\n",
    "# Cleaned Layer: Remove NULLs\n",
    "@dlt.table(name=\"orders_cleaned\")\n",
    "def clean_orders():\n",
    "    return dlt.read(\"orders_raw\").dropna()\n",
    "\n",
    "# Aggregated Layer: Revenue per Category\n",
    "@dlt.table(name=\"revenue_by_category\")\n",
    "def aggregate_revenue():\n",
    "    orders = dlt.read(\"orders_cleaned\")\n",
    "    products = dlt.read(\"products_raw\")\n",
    "    joined = orders.join(products, on=\"ProductID\", how=\"inner\")\n",
    "    return joined.groupBy(\"Category\").agg(expr(\"SUM(Quantity * Price) AS TotalRevenue\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2da5e9-6a45-4444-b782-7099c17337fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation   |operationParameters                                                                                                                                                                                      |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|5      |2025-06-19 10:32:22|3359264194789707|azuser3558_mml.local@techademy.com|MERGE       |{predicate -> [\"(cast(OrderID#55185 as bigint) = OrderID#55923L)\"], matchedPredicates -> [], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{4356610334858723}|0611-041524-ifuhl15z|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1794, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 1662, materializeSourceTimeMs -> 4, numTargetRowsInserted -> 1, conflictDetectionTimeMs -> 20, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1632}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2025-06-19 10:32:21|3359264194789707|azuser3558_mml.local@techademy.com|OPTIMIZE    |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                           |NULL|{4356610334858723}|0611-041524-ifuhl15z|3          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3908, p25FileSize -> 2075, numDeletionVectorsRemoved -> 1, minFileSize -> 2075, numAddedFiles -> 1, maxFileSize -> 2075, p75FileSize -> 2075, p50FileSize -> 2075, numAddedBytes -> 2075}                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2025-06-19 10:32:19|3359264194789707|azuser3558_mml.local@techademy.com|UPDATE      |{predicate -> [\"(Status#55191 = Pending)\"]}                                                                                                                                                              |NULL|{4356610334858723}|0611-041524-ifuhl15z|2          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 946, numDeletionVectorsUpdated -> 0, scanTimeMs -> 485, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1834, rewriteTimeMs -> 461}                                                                                                                                                                                                                                                                                                                                                                                      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-19 10:32:04|3359264194789707|azuser3558_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                             |NULL|{4356610334858723}|0611-041524-ifuhl15z|1          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2074}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-19 10:31:51|3359264194789707|azuser3558_mml.local@techademy.com|WRITE       |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                             |NULL|{4356610334858723}|0611-041524-ifuhl15z|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2074}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-19 10:26:30|3359264194789707|azuser3558_mml.local@techademy.com|CREATE TABLE|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}                                                |NULL|{4356610334858723}|0611-041524-ifuhl15z|NULL       |WriteSerializable|true         |{}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Time Travel\n",
    "# 7. View data before the Status update.\n",
    "\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/delta/orders`\").show(truncate=False)\n",
    "\n",
    "# 8. Restore to an older version of the orders table.\n",
    "\n",
    "old_df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"/delta/orders\")\n",
    "\n",
    "old_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9e659a-2e27-41bf-ba60-20a8eda7d92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 9. Run VACUUM after changing default retention.\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "spark.sql(\"VACUUM delta.`/delta/orders` RETAIN 0 HOURS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6431620d-c360-4643-996f-fedd13710edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>orders_cleaned</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>ProductID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>date</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Status</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderType</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=641763771411399#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser3558_mml.local%40techademy.com%2FJune_17_Assignment_1&redirectNotebookId=4356610334858723\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10. Quantity > 0 , Price > 0 , OrderDate is not null\n",
    "\n",
    "# 11. Use when-otherwise to create a new column: OrderType = \"Return\" if Status =='Returned'\n",
    "import dlt\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "@dlt.table(name=\"orders_cleaned\")\n",
    "@dlt.expect(\"valid_quantity\", \"Quantity > 0\")\n",
    "@dlt.expect(\"valid_price\", \"Price > 0\")\n",
    "@dlt.expect(\"order_date_not_null\", \"OrderDate IS NOT NULL\")\n",
    "def orders_cleaned():\n",
    "    df = dlt.read(\"orders_raw\").dropna()\n",
    "    \n",
    "    # Add derived column\n",
    "    df = df.withColumn(\n",
    "        \"OrderType\",\n",
    "        when(df[\"Status\"] == \"Returned\", \"Return\").otherwise(\"Normal\")\n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "June_17_Assignment_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}