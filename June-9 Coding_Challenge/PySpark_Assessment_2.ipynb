{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V7DXn9c8jlea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4d3460-0f00-46c7-c526-c60a0a0b891e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "employees_df = spark.read.csv(\"employees.csv\",header=True, inferSchema=True)\n",
        "attendance_df = spark.read.csv(\"attendance.csv\", header=True,inferSchema=True)\n",
        "bonuses_df = spark.read.json(\"bonuses.json\")\n",
        "\n",
        "#Show schemas and sample records\n",
        "\n",
        "print(\"Employees Schema:\")\n",
        "employees_df.printSchema()\n",
        "employees_df.show()\n",
        "\n",
        "print(\"Attendance Schema:\")\n",
        "attendance_df.printSchema()\n",
        "attendance_df.show()\n",
        "\n",
        "print(\"Bonuses Schema:\")\n",
        "bonuses_df.printSchema()\n",
        "bonuses_df.show()\n",
        "\n",
        "#Count distinct departments\n",
        "print(\"Distinct departments:\",employees_df.select(\"Department\").distinct().count())\n",
        "\n"
      ],
      "metadata": {
        "id": "gQztJ6apkAr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fb5de7-5952-41ab-e17e-b1f4399d959a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees Schema:\n",
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- JoinDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- ManagerID: integer (nullable = true)\n",
            "\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|\n",
            "+-----+------+-----------+----------+------+---------+\n",
            "\n",
            "Attendance Schema:\n",
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            "\n",
            "+-----+----------+-------+\n",
            "|EmpID|      Date| Status|\n",
            "+-----+----------+-------+\n",
            "|    1|2024-04-01|Present|\n",
            "|    1|2024-04-02|Present|\n",
            "|    2|2024-04-01| Absent|\n",
            "|    2|2024-04-02|Present|\n",
            "|    3|2024-04-01|Present|\n",
            "|    3|2024-04-02|Present|\n",
            "|    4|2024-04-01| Absent|\n",
            "|    4|2024-04-02| Absent|\n",
            "|    5|2024-04-01|Present|\n",
            "|    5|2024-04-02|Present|\n",
            "+-----+----------+-------+\n",
            "\n",
            "Bonuses Schema:\n",
            "root\n",
            " |-- Bonus: long (nullable = true)\n",
            " |-- EmpID: long (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n",
            "+-----+-----+----+---------------+\n",
            "|Bonus|EmpID|Year|_corrupt_record|\n",
            "+-----+-----+----+---------------+\n",
            "| NULL| NULL|NULL|              [|\n",
            "| 5000|    1|2023|           NULL|\n",
            "| 7000|    2|2023|           NULL|\n",
            "| 6500|    3|2023|           NULL|\n",
            "| 6000|    4|2023|           NULL|\n",
            "| 4000|    5|2023|           NULL|\n",
            "| NULL| NULL|NULL|              ]|\n",
            "+-----+-----+----+---------------+\n",
            "\n",
            "Distinct departments: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.DataFrame Operations\n",
        "from pyspark.sql.functions import to_date, datediff, current_date, round, col\n",
        "\n",
        "#  Ensure JoinDate is in proper date format\n",
        "# employees_df = employees_df.withColumn(\"JoinDate\", to_date(col(\"JoinDate\")))\n",
        "\n",
        "# Add TenureYears using datediff() and round()\n",
        "employees_df = employees_df.withColumn(\n",
        "    \"TenureYears\", round(datediff(current_date(), col(\"JoinDate\")) / 365, 2)\n",
        ")\n",
        "\n",
        "#  Calculate TotalCompensation = Salary + Bonus\n",
        "\n",
        "emp_bonus = employees_df.join(bonuses_df, on=\"EmpID\", how=\"left\")\n",
        "emp_bonus = emp_bonus.withColumn(\n",
        "    \"TotalCompensation\",col(\"Salary\") + col(\"Bonus\")\n",
        ")\n",
        "\n",
        "# Filter employees with more than 2 years in the company\n",
        "emp_bonus.filter(col(\"TenureYears\") > 2).show()\n",
        "\n",
        "# Show employees who report to a manager (ManagerID is not null)\n",
        "manager = emp_bonus.filter(col(\"ManagerID\").isNotNull())\n",
        "manager.select(\"EmpID\",\"Name\").show()\n"
      ],
      "metadata": {
        "id": "oFOemvmVkgj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaea3c1-6348-44c3-f2c1-658fabab5dfb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+------+---------+-----------+-----+----+---------------+-----------------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|TenureYears|Bonus|Year|_corrupt_record|TotalCompensation|\n",
            "+-----+------+-----------+----------+------+---------+-----------+-----+----+---------------+-----------------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|       4.11| 5000|2023|           NULL|            60000|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|       5.24| 7000|2023|           NULL|            87000|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|       2.92| 6500|2023|           NULL|            81500|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|       5.56| 6000|2023|           NULL|            66000|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|       2.43| 4000|2023|           NULL|            54000|\n",
            "+-----+------+-----------+----------+------+---------+-----------+-----+----+---------------+-----------------+\n",
            "\n",
            "+-----+------+\n",
            "|EmpID|  Name|\n",
            "+-----+------+\n",
            "|    2|   Raj|\n",
            "|    3|Simran|\n",
            "|    4| Aamir|\n",
            "|    5| Nisha|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Aggregation\n",
        "# Average salary per department\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "employees_df.groupBy(\"Department\").agg(round(avg(\"Salary\"), 2).alias(\"AvgSalary\")).show()\n",
        "\n",
        "# Number of employees under each manager\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "employees_df.groupBy(\"ManagerID\").agg( count(\"EmpID\").alias(\"Count\")).filter(\"ManagerID IS NOT NULL\").show()\n",
        "\n",
        "# Count of absences per employee\n",
        "from pyspark.sql.functions import count, when\n",
        "absences_df = attendance_df.filter(col(\"Status\") == \"Absent\")\n",
        "\n",
        "absences_df.groupBy(\"EmpID\").agg(count(\"*\").alias(\"Days\")\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "UNVsa-DbkqsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06320d39-ba26-4dda-fa60-eabe44d46c25"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "| Department|AvgSalary|\n",
            "+-----------+---------+\n",
            "|Engineering|  77500.0|\n",
            "|         HR|  52500.0|\n",
            "|  Marketing|  60000.0|\n",
            "+-----------+---------+\n",
            "\n",
            "+---------+-----+\n",
            "|ManagerID|Count|\n",
            "+---------+-----+\n",
            "|        1|    4|\n",
            "+---------+-----+\n",
            "\n",
            "+-----+----+\n",
            "|EmpID|Days|\n",
            "+-----+----+\n",
            "|    4|   2|\n",
            "|    2|   1|\n",
            "+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  4. Joins\n",
        "# Join employees and attendance → Get attendance % (Present days / Total days).\n",
        "\n",
        "from pyspark.sql.functions import count, when, col, round\n",
        "\n",
        "attendance_data = attendance_df.groupBy(\"EmpID\").agg(\n",
        "    count(\"*\").alias(\"TotalDays\"),\n",
        "    count(when(col(\"Status\") == \"Present\", True)).alias(\"PresentDays\")\n",
        ").withColumn(\n",
        "    \"AttendancePercent\", round((col(\"PresentDays\") / col(\"TotalDays\")) * 100, 2)\n",
        ")\n",
        "\n",
        "emp_attendance = employees_df.join(attendance_data, on=\"EmpID\", how=\"left\")\n",
        "emp_attendance.select(\"EmpID\", \"Name\", \"Department\", \"AttendancePercent\").show()\n"
      ],
      "metadata": {
        "id": "mICaul3_kthP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58dd50f-c112-4eac-fd44-b7fa9b805787"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+-----------------+\n",
            "|EmpID|  Name| Department|AttendancePercent|\n",
            "+-----+------+-----------+-----------------+\n",
            "|    1| Anita|         HR|            100.0|\n",
            "|    2|   Raj|Engineering|             50.0|\n",
            "|    3|Simran|Engineering|            100.0|\n",
            "|    4| Aamir|  Marketing|              0.0|\n",
            "|    5| Nisha|         HR|            100.0|\n",
            "+-----+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join employees and bonuses -> Show top 3 employees by TotalCompensatio\n",
        "\n",
        "emp_bonus = employees_df.join(bonuses_df, on=\"EmpID\", how=\"left\")\n",
        "\n",
        "emp_bonus = emp_bonus.withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\"))\n",
        "\n",
        "emp_bonus.orderBy(col(\"TotalCompensation\").desc()).select(\n",
        "    \"EmpID\", \"Name\", \"TotalCompensation\").show(3)\n"
      ],
      "metadata": {
        "id": "mYXEpGR9ktXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8f61a6-4bc7-48e2-956e-a25267b7df6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------------+\n",
            "|EmpID|  Name|TotalCompensation|\n",
            "+-----+------+-----------------+\n",
            "|    2|   Raj|            87000|\n",
            "|    3|Simran|            81500|\n",
            "|    4| Aamir|            66000|\n",
            "+-----+------+-----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi-level join: employees + bonuses + attendance\n",
        "multi_join = employees_df.join(bonuses_df, on=\"EmpID\", how=\"left\") \\\n",
        "    .join(attendance_data, on=\"EmpID\", how=\"left\")\n",
        "\n",
        "multi_join = multi_join.withColumn(\"TotalCompensation\", col(\"Salary\") + col(\"Bonus\"))\n",
        "\n",
        "multi_join.select(\"EmpID\", \"Name\", \"Department\", \"TotalCompensation\", \"AttendancePercent\").show()\n"
      ],
      "metadata": {
        "id": "8wDgM6mKk9j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa8f3a9-2f65-4cb2-e09f-e553bd16fb5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+-----------------+-----------------+\n",
            "|EmpID|  Name| Department|TotalCompensation|AttendancePercent|\n",
            "+-----+------+-----------+-----------------+-----------------+\n",
            "|    1| Anita|         HR|            60000|            100.0|\n",
            "|    2|   Raj|Engineering|            87000|             50.0|\n",
            "|    3|Simran|Engineering|            81500|            100.0|\n",
            "|    4| Aamir|  Marketing|            66000|              0.0|\n",
            "|    5| Nisha|         HR|            54000|            100.0|\n",
            "+-----+------+-----------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, substring, concat, lit, lpad, col\n",
        "\n",
        "# 1. Extract year and month from JoinDate\n",
        "date_info = employees_df.withColumn(\"Year\", year(\"JoinDate\")) .withColumn(\"Month\", month(\"JoinDate\"))\n",
        "\n",
        "date_info.select(\"EmpID\", \"Name\", \"JoinDate\", \"Year\", \"Month\").show()\n",
        "\n",
        "# 2. Mask employee names (e.g., \"Anita\" → \"A****\")\n",
        "emp_mask = date_info.withColumn(\"MaskedName\", concat(substring(\"Name\", 1, 1), lit(\"****\")))\n",
        "emp_mask.select(\"EmpID\", \"Name\", \"MaskedName\").show()\n",
        "\n",
        "# 3. Use substring/lpad to create EmpCode like \"EMP001\"\n",
        "emp_code = emp_mask.withColumn( \"EmpCode\", concat(lit(\"EMP\"), lpad(col(\"EmpID\").cast(\"string\"), 3, \"0\")))\n",
        "emp_code.select(\"EmpID\", \"EmpCode\", \"Name\").show()\n"
      ],
      "metadata": {
        "id": "5CmPVN6ilAH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4cc2fb-c7ab-49ec-c02a-b580b6ea2207"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+----+-----+\n",
            "|EmpID|  Name|  JoinDate|Year|Month|\n",
            "+-----+------+----------+----+-----+\n",
            "|    1| Anita|2021-05-01|2021|    5|\n",
            "|    2|   Raj|2020-03-15|2020|    3|\n",
            "|    3|Simran|2022-07-10|2022|    7|\n",
            "|    4| Aamir|2019-11-20|2019|   11|\n",
            "|    5| Nisha|2023-01-05|2023|    1|\n",
            "+-----+------+----------+----+-----+\n",
            "\n",
            "+-----+------+----------+\n",
            "|EmpID|  Name|MaskedName|\n",
            "+-----+------+----------+\n",
            "|    1| Anita|     A****|\n",
            "|    2|   Raj|     R****|\n",
            "|    3|Simran|     S****|\n",
            "|    4| Aamir|     A****|\n",
            "|    5| Nisha|     N****|\n",
            "+-----+------+----------+\n",
            "\n",
            "+-----+-------+------+\n",
            "|EmpID|EmpCode|  Name|\n",
            "+-----+-------+------+\n",
            "|    1| EMP001| Anita|\n",
            "|    2| EMP002|   Raj|\n",
            "|    3| EMP003|Simran|\n",
            "|    4| EMP004| Aamir|\n",
            "|    5| EMP005| Nisha|\n",
            "+-----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Conditional & Null Handling\n",
        "# Use when/otherwise to label performance:\n",
        "# “High” if Bonus > 6000\n",
        "# “Medium” if 4000–6000\n",
        "# “Low” otherwise\n",
        "\n",
        "emp_bonus_df = employees_df.join(bonuses_df, on=\"EmpID\", how=\"left\")\n",
        "\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "emp_performance = emp_bonus_df.withColumn(\n",
        "    \"PerformanceLabel\",\n",
        "    when(col(\"Bonus\") > 6000, \"High\")\n",
        "    .when((col(\"Bonus\") >= 4000) & (col(\"Bonus\") <= 6000), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "emp_performance.select(\"EmpID\", \"Name\", \"Bonus\", \"PerformanceLabel\").show()\n",
        "\n",
        "# Handle missing ManagerID using fillna(\"No Manager\")\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "missing = emp_performance.withColumn(\"ManagerID\", col(\"ManagerID\").cast(\"string\")).fillna({\"ManagerID\": \"No Manager\"})\n",
        "\n",
        "missing.select(\"EmpID\", \"Name\", \"ManagerID\").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1FlVBWbklT0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1647ca54-07dc-49d7-c3d1-682d685fe0ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----+----------------+\n",
            "|EmpID|  Name|Bonus|PerformanceLabel|\n",
            "+-----+------+-----+----------------+\n",
            "|    1| Anita| 5000|          Medium|\n",
            "|    2|   Raj| 7000|            High|\n",
            "|    3|Simran| 6500|            High|\n",
            "|    4| Aamir| 6000|          Medium|\n",
            "|    5| Nisha| 4000|          Medium|\n",
            "+-----+------+-----+----------------+\n",
            "\n",
            "+-----+------+----------+\n",
            "|EmpID|  Name| ManagerID|\n",
            "+-----+------+----------+\n",
            "|    1| Anita|No Manager|\n",
            "|    2|   Raj|         1|\n",
            "|    3|Simran|         1|\n",
            "|    4| Aamir|         1|\n",
            "|    5| Nisha|         1|\n",
            "+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  7. Spark SQL\n",
        "# Create and use database hr\n",
        "# Save all DataFrames as tables: employees , attendance , bonuses\n",
        "\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS hr\")\n",
        "spark.sql(\"USE hr\")\n",
        "\n",
        "employees_df.write.mode(\"overwrite\").saveAsTable(\"employees\")\n",
        "attendance_df.write.mode(\"overwrite\").saveAsTable(\"attendance\")\n",
        "bonuses_df.write.mode(\"overwrite\").saveAsTable(\"bonuses\")\n"
      ],
      "metadata": {
        "id": "8J48X_JSliQ2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write SQL queries:\n",
        "# Top paid employee in each department.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT Department, Name, Salary\n",
        "FROM (SELECT Department, Name, Salary,\n",
        "           RANK() OVER (PARTITION BY Department ORDER BY Salary DESC) as rank FROM employees\n",
        ")\n",
        "WHERE rank = 1\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "9iAS5bMblxGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289a307a-0dd4-44f5-ab07-00067e69b08f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+------+\n",
            "| Department| Name|Salary|\n",
            "+-----------+-----+------+\n",
            "|Engineering|  Raj| 80000|\n",
            "|         HR|Anita| 55000|\n",
            "|  Marketing|Aamir| 60000|\n",
            "+-----------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attendance rate by department.\n",
        "spark.sql(\"\"\"\n",
        "SELECT e.Department,\n",
        "       ROUND(SUM(CASE WHEN a.Status = 'Present' THEN 1 ELSE 0 END) / COUNT(*), 2) AS AttendanceRate\n",
        "FROM employees e\n",
        "JOIN attendance a ON e.EmpID = a.EmpID\n",
        "GROUP BY e.Department\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "o9yBzB7xlxCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6177d8-a9e9-4d4e-e15c-74f45b8eab7a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "| Department|AttendanceRate|\n",
            "+-----------+--------------+\n",
            "|Engineering|          0.75|\n",
            "|         HR|           1.0|\n",
            "|  Marketing|           0.0|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Employees joined after 2021 with salary > 70,000\n",
        "spark.sql(\"\"\"\n",
        "SELECT EmpID, Name, Department, JoinDate, Salary\n",
        "FROM employees\n",
        "WHERE JoinDate > '2021-12-31' AND Salary > 70000\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "Vi3V2CIglw7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c08d75-62d9-41cf-b793-7bb9e308e91d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|\n",
            "+-----+------+-----------+----------+------+\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|\n",
            "+-----+------+-----------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Advanced (Optional)\n",
        "# Use a UDF to classify department as \"Tech\" vs \"Non-Tech\"\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def classify_dept(dept):\n",
        "    return \"Tech\" if dept.lower() == \"engineering\" else \"Non-Tech\"\n",
        "\n",
        "classify_udf = udf(classify_dept, StringType())\n",
        "\n",
        "emp_classify = employees_df.withColumn(\"DeptType\", classify_udf(col(\"Department\")))\n",
        "emp_classify.show()\n"
      ],
      "metadata": {
        "id": "B2Scxg5Hl5WC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d706a99-b35d-450e-c797-6a7f0c2a3ee2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+------+---------+-----------+--------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|TenureYears|DeptType|\n",
            "+-----+------+-----------+----------+------+---------+-----------+--------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|       4.11|Non-Tech|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|       5.24|    Tech|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|       2.92|    Tech|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|       5.56|Non-Tech|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|       2.43|Non-Tech|\n",
            "+-----+------+-----------+----------+------+---------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, sum as _sum\n",
        "\n",
        "# Create a view emp_attendance_summary\n",
        "attendance_data = attendance_df.groupBy(\"EmpID\").agg(\n",
        "        count(\"*\").alias(\"TotalDays\"),\n",
        "        _sum(when(col(\"Status\") == \"Present\", 1).otherwise(0)).alias(\"PresentDays\")\n",
        "    ).withColumn(\"AttendanceRate\", (col(\"PresentDays\") / col(\"TotalDays\")).cast(\"double\"))\n",
        "\n",
        "emp_attendance_summary = employees_df.join(attendance_data, \"EmpID\", \"left\")\n",
        "\n",
        "emp_attendance_summary.createOrReplaceTempView(\"emp_attendance_summary\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM emp_attendance_summary\").show()\n"
      ],
      "metadata": {
        "id": "sNqP_khjl8ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1e8ca2-b82d-44fe-d399-7551db94c24e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+----------+------+---------+-----------+---------+-----------+--------------+\n",
            "|EmpID|  Name| Department|  JoinDate|Salary|ManagerID|TenureYears|TotalDays|PresentDays|AttendanceRate|\n",
            "+-----+------+-----------+----------+------+---------+-----------+---------+-----------+--------------+\n",
            "|    1| Anita|         HR|2021-05-01| 55000|     NULL|       4.11|        2|          2|           1.0|\n",
            "|    2|   Raj|Engineering|2020-03-15| 80000|        1|       5.24|        2|          1|           0.5|\n",
            "|    3|Simran|Engineering|2022-07-10| 75000|        1|       2.92|        2|          2|           1.0|\n",
            "|    4| Aamir|  Marketing|2019-11-20| 60000|        1|       5.56|        2|          0|           0.0|\n",
            "|    5| Nisha|         HR|2023-01-05| 50000|        1|       2.43|        2|          2|           1.0|\n",
            "+-----+------+-----------+----------+------+---------+-----------+---------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save it as Parquet partitioned by Department\n",
        "emp_attendance_summary.write .mode(\"overwrite\").partitionBy(\"Department\").parquet(\"emp_attendance_summary_parquet\")\n",
        "parquet_df = spark.read.parquet(\"emp_attendance_summary_parquet\")\n",
        "parquet_df.printSchema()\n",
        "parquet_df.show()\n"
      ],
      "metadata": {
        "id": "WkhtmZppmADw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519debe1-5a72-47fc-923e-ecd2a7911d09"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- JoinDate: date (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- ManagerID: integer (nullable = true)\n",
            " |-- TenureYears: double (nullable = true)\n",
            " |-- TotalDays: long (nullable = true)\n",
            " |-- PresentDays: long (nullable = true)\n",
            " |-- AttendanceRate: double (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            "\n",
            "+-----+------+----------+------+---------+-----------+---------+-----------+--------------+-----------+\n",
            "|EmpID|  Name|  JoinDate|Salary|ManagerID|TenureYears|TotalDays|PresentDays|AttendanceRate| Department|\n",
            "+-----+------+----------+------+---------+-----------+---------+-----------+--------------+-----------+\n",
            "|    1| Anita|2021-05-01| 55000|     NULL|       4.11|        2|          2|           1.0|         HR|\n",
            "|    5| Nisha|2023-01-05| 50000|        1|       2.43|        2|          2|           1.0|         HR|\n",
            "|    2|   Raj|2020-03-15| 80000|        1|       5.24|        2|          1|           0.5|Engineering|\n",
            "|    3|Simran|2022-07-10| 75000|        1|       2.92|        2|          2|           1.0|Engineering|\n",
            "|    4| Aamir|2019-11-20| 60000|        1|       5.56|        2|          0|           0.0|  Marketing|\n",
            "+-----+------+----------+------+---------+-----------+---------+-----------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}